# -*- coding: utf-8 -*-
"""Great Customer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u4Q6XpKHt_yvrSPfH0Ko0qt3DZnx1kb5
"""

# from google.colab import files
 
 #uploaded = files.upload()

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objs as go
import plotly.express as px
import plotly.subplots as sp
from plotly.subplots import make_subplots
import plotly.figure_factory as ff


from scipy.stats import f_oneway
from scipy.stats import f
from scipy.stats import chi2
from scipy.stats import chisquare
from scipy.stats import chi2_contingency
from scipy.stats import norm
from scipy.stats import ttest_ind


from typing import List

pd.set_option('display.max_columns', None)

"""## **Reading Datasets**"""

customer = pd.read_csv('great_customers.csv')

g_c = customer.copy()

"""## **Checking the Datasets**"""

g_c.shape

g_c.info()

g_c.head()

g_c['great_customer_class'].unique()

g_c.occupation.value_counts().sort_values(ascending = False)

"""## **Null Value Percentage**"""

g_c_null_colms = g_c.isnull().mean() * 100
g_c_null_colms

"""## **Visualizing the Null Values**"""

fig = px.imshow(g_c.isnull(), color_continuous_scale='thermal')

fig.update_layout(
    
    title = {
        'text' : "Destribution of Null Values",
        'x' : 0.5,
        'y' : 0.98
    },
    width = 900,
)
fig.show()

"""## **Handling missing values of numerical columns**

### **Columns Having Nulls**
"""

num_cols_with_nulls = []
for col in g_c.columns:
  if g_c_null_colms[col] > 0:
    if g_c[col].dtype == 'int64' or g_c[col].dtype == 'float64':
      num_cols_with_nulls.append(col)

num_cols_with_nulls

"""#### **Plotting the Distribution**"""

def num_col_dest(rows, cols, df, cols_list, title, break_point = -1, height = 600, width = 1200):

  fig = sp.make_subplots(rows=rows, cols=cols)

  idx  = 0


  for r in range(1, rows + 1):
    for c in range(1, cols + 1):

      col = cols_list[idx]
      idx += 1

      data = df[col]

      # data.rename(columns = {'index': col, col : 'frequency'}, inplace = True)


      trace = go.Histogram(x=data, nbinsx=20)

      fig.add_trace(trace, row = r, col = c)

      fig.update_xaxes(title_text=col, row=r, col=c)
      fig.update_yaxes(title_text="frequency", row=r, col=c)

      if idx == break_point: break



  fig.update_layout(
      title={
          'text': title,
          'y': 0.989,
          'x': 0.5,
          'xanchor': 'center',
          'yanchor': 'top',
          'font' : {
              'color' : '#393646',
              'family' : 'Bold', 
              'size' : 26
          } 
          
      },
      
      showlegend = False, 
      height = height,
      width = width
  )

  fig.update_xaxes(range = [0, 170000], row = 1, col = 2)
  fig.update_xaxes(range = [-1200, 12000], row = 1, col = 3)
  fig.update_xaxes(range = [-500, 2500], row = 2, col = 1)

  # fig.update_yaxes(range = [0, 2000], row = 2, col = 1)
  # fig.update_yaxes(range = [0, 2000], row = 1, col = 3)



  fig.show()

num_col_dest(2, 3, g_c, num_cols_with_nulls,  "Destribution of numericsl features having NA's")

"""#### **Using Mean and Median to fill up NA's**"""

# Numerical Columns Having Null values
# ['age', 'salary', 'mins_beerdrinking_year', 'mins_exercising_year', 'tea_per_year', 'coffee_per_year']

solve_col = []

g_c_no_null = g_c.copy()

for col in num_cols_with_nulls:
  
  
  mean = g_c_no_null[col].mean()
  median = g_c_no_null[col].median()

  # Create new columns with the original column name and "_mean" or "_median" suffix
  # The new columns contain the original values with missing values replaced by the mean or median

  g_c_no_null[col + "_mean"] = g_c_no_null[col].fillna(mean)
  g_c_no_null[col + "_median"] = g_c_no_null[col].fillna(median)

  solve_col.append(col)
  solve_col.append(col + "_mean")
  solve_col.append(col + "_median")

"""#### **Checking the Basic Statistical Difference**"""

g_c_no_null[solve_col].describe()

"""#### **Checking the Variance Difference**"""

idx = 1

original = g_c_no_null[solve_col[0]].var()

for col in solve_col:
  change = 0
  variance = g_c_no_null[col].var()
  
  prct = 100 - (variance / original) * 100

  print(f"{col} Variance: {variance:.2f} ({prct:.2f}% Difference)")
  
  if idx % 3 == 0:
    change = 1
    print(" ")

  idx += 1

  if change and idx < len(solve_col):
    original = g_c_no_null[solve_col[idx - 1]].var()

"""#### **Plotting the Distribution Difference**

#### **Based on the distribution analysis, it is observed that there are certain columns that demonstrate less variation in distribution after filling the missing values with the mean. These columns include Mins_BeerDrinking_Year, Mins_Exercising_Year, and Coffee_Per_Year.**

#### **On the other hand, there are other columns such as Age, Salary, and Tea_Per_Year, where the distribution shows significantly less variation after filling the missing values with the median. Therefore, we have to use the median to fill the missing values in these columns.**
"""

fig, axs = plt.subplots(2, 3, figsize=(16, 8))


idx = 0

for r in range(2):
  for c in range(3):

    col = num_cols_with_nulls[idx]
    idx+=1

    sns.kdeplot(data=g_c_no_null[col], ax=axs[r, c], label="Original")
    sns.kdeplot(data=g_c_no_null[col + "_mean"], ax=axs[r, c], label=col.title() + " Mean")
    sns.kdeplot(data=g_c_no_null[col + "_median"], ax=axs[r, c], label=col.title() + " Median")
    axs[r, c].set_title(col.upper() + " Distribution")
    axs[r, c].set_xlabel(col.upper())
    axs[r, c].legend()


axs[0, 1].set_xlim(0, 150000)


plt.tight_layout(pad = 3.0)
plt.show()

mean_fill = ['mins_beerdrinking_year', 'mins_exercising_year', 'coffee_per_year']
median_fill = ['age', 'salary', 'tea_per_year']

g_c_no_null[mean_fill] = g_c_no_null[['mins_beerdrinking_year_mean', 'mins_exercising_year_mean', 'coffee_per_year_mean']]
g_c_no_null[median_fill] = g_c_no_null[['age_median', 'salary_median', 'tea_per_year_median']]

cols = g_c.columns

g_c_no_null = g_c_no_null[cols]

g_c_no_null.isnull().mean() * 100

"""## **Handling the missing values of Categorical columns**

#### **Null Value Percentage**
"""

null_catg_cols = []

# Extracting the categorical cols having null values
 
for col in g_c.columns:
  if g_c_null_colms[col] > 0:
    if g_c[col].dtype != 'int64' and g_c[col].dtype != 'float64':
      print(f"{col} : {g_c_null_colms[col]:.2f}%")
      null_catg_cols.append(col)

"""#### **Plotting the Destribution**"""

def catg_col_dest(rows, cols, catg_cols, break_point = -1, title = "Distribution of Categorical Features Having Null Values"):

  fig = sp.make_subplots(rows=rows, cols=cols)

  idx  = 0


  for r in range(1, rows + 1):
    for c in range(1, cols + 1):

      col = catg_cols[idx]
      idx += 1

      data = g_c_no_null[col].value_counts().sort_values(ascending = False).reset_index()

      data.rename(columns = {'index': col, col : 'frequency'}, inplace = True)


      trace = go.Bar(x = data[col], y = data['frequency'])

      fig.add_trace(trace, row = r, col = c)

      fig.update_xaxes(title_text=col, row=r, col=c)
      fig.update_yaxes(title_text="frequency", row=r, col=c)

      if idx == break_point: break



  fig.update_layout(
      title={
          'text': title,
          'y': 0.989,
          'x': 0.5,
          'xanchor': 'center',
          'yanchor': 'top',
          'font' : {
              'color' : '#393646',
              'family' : 'Bold', 
              'size' : 26
          } 
          
      },
      
      showlegend = False, 
      height = 600,
      width = 1200
  )


  fig.show()

catg_col_dest(1, 2, null_catg_cols)

"""### **Using Mode to fill NA's**


"""

catg_null_cols = ['workclass', 'occupation']


for col in catg_null_cols:

  mode = g_c_no_null[col].mode()[0]

  g_c_no_null[col] = g_c_no_null[col].fillna(mode)

catg_col_dest(1, 2, null_catg_cols, title = "Distribution of Categorical Features after Handling Null Values")

g_c_no_null.isnull().mean() * 100

"""## **Using Encoding to convert the string of categorical features to numeric**"""

cols_to_transforme = []

for col in g_c.columns:

  if g_c[col].dtype != 'int64' and g_c[col].dtype != 'float64':
    cols_to_transforme.append(col)

cols_to_transforme

"""#### **Number of unique category in each feature**"""

for col in cols_to_transforme:
  print(col)
  print(g_c_no_null[col].unique())
  print()

catg_col_dest(2, 3, cols_to_transforme, break_point = 5, title = "Distribution of Categorical Features")

"""#### **From the above plot we can see there are too many categories in occupation feature. If we us one hot encode in this feature it will generate a lot of new features resulting a curse of dimensionality and increase computational complexity.**

#### **As a result we have to use normal one hot encode to other features other then occupation**

#### **Coding our own one hot encoding function**
"""

def one_hot_encode(df):

    feature_len = len(df.columns)

    for feature in df.columns:

        # Loop through each unique category in the feature
        for catg in df[feature].unique():
            # Creating a new column for each category using one-hot encoding
            # by checking if the category is equal to the feature and
            # converting the resulting boolean values to integers
            df[f"{feature}_{catg}"] = (df[feature] == catg).astype('int32')

    # Returning only the newly created columns
    return df.iloc[:, feature_len : ]

"""#### **Performing One Hot Encoding Technique**"""

g_c.head(1)

cols_to_one_hot_encode = ['workclass', 'marital-status', 'race', 'sex']

g_c_one_encode = g_c_no_null[cols_to_one_hot_encode].copy()

g_c_one_encode = one_hot_encode(g_c_one_encode)


cols_to_add = ['user_id', 'age', 'salary', 'education_rank', 'occupation', 'mins_beerdrinking_year',
               'mins_exercising_year', 'works_hours', 'tea_per_year', 'coffee_per_year', 'great_customer_class']

g_c_one_encode = pd.concat([g_c_one_encode, g_c_no_null[cols_to_add]], axis = 1)

g_c_one_encode

"""### **Handling the occupation feature**"""

g_c_one_encode['occupation'].unique()

data = g_c_one_encode['occupation'].value_counts().reset_index()

# plot the histogram using plotly.express
fig = px.bar(data, x='index', y='occupation', labels={'index':'Occupation', 'occupation':'Count'})

fig.update_layout(
    
    title = {
        'text': "Occupation",
        'y': 0.989,
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font' : {
            'color' : '#393646',
            'family' : 'Bold', 
            'size' : 26
        },
    },
    height = 500,
    width  = 1200
)

fig.show()

"""Grouping the categories for the occupation feature:

*   Office jobs: clerical, professional, executive, lawenf, estate_agent
*   Manual jobs: farm, craft, factory, cleaner, soldier
*   Sales jobs: sales
*   Service jobs: service
*   Driver jobs: trucker
*   Tech jobs: tech




"""

job_catgories = {
    'office_jobs': ['clerical', 'professional', 'executive', 'lawenf', 'estate_agent'],
    'manual_jobs': ['farm', 'craft', 'factory', 'cleaner', 'soldier', 'trucker'],
    'sales_jobs' : ['sales'],
    'service_jobs' : ['service'],
    'Tech_jobs' : ['tech']
}

# 'office_jobs', 'manual_jobs', 'sales_jobs', 'service_jobs', 'Tech_jobs'

g_c_encode = g_c_one_encode.copy()

for catg in job_catgories:
  g_c_encode[catg] = g_c_encode['occupation'].isin(job_catgories[catg]).astype('int32')

g_c_encode.sample(5)

re_index_cols = ['user_id', 'age','office_jobs', 'manual_jobs', 'sales_jobs', 'service_jobs', 'Tech_jobs',
                 'workclass_private', 'workclass_government', 'salary', 'education_rank', 'marital-status_Divorced', 
                 'marital-status_Married', 'marital-status_Widowed', 'race_caucasian', 'sex_Male', 
                 'mins_beerdrinking_year', 'mins_exercising_year', 'works_hours', 'tea_per_year', 'coffee_per_year', 
                 'great_customer_class']

g_c_encode = g_c_encode.reindex(re_index_cols, axis = 1)
g_c_encode.head()

g_c_encode.shape

g_c_encode.columns

g_c_encode.head()

"""## **Handling Outliers**

### **Plotting the destribution to identify any outliers**
"""

num_colms = ['age', 'salary', 'mins_beerdrinking_year', 'mins_exercising_year', 'works_hours', 'tea_per_year', 'coffee_per_year']

# , vertical_spacing = 0.14, horizontal_spacing = 0.05
fig = sp.make_subplots(rows=2, cols=4)

idx = 0

for r in range(1, 3):
  for c in range(1, 5):

    col = num_colms[idx]
    idx+=1

    trace = go.Box(x=g_c_no_null[col], orientation='h', boxpoints='suspectedoutliers')

    fig.add_trace(trace, row=r, col=c)
    fig.update_yaxes(showticklabels=False)
    fig.update_xaxes(title_text=col, row=r, col=c)

    if idx == 7: break


fig.update_layout(
    title={
        'text': 'Destribution of Numeric Columns',
        'y': 0.989,
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font' : {
            'color' : '#393646',
            'family' : 'Bold', 
            'size' : 26
        } 
        
    },

     
    showlegend = False, 
    height = 800,
    width = 1200
)


fig.update_xaxes(range=[0, 170000], row=1, col=2) 
# fig.update_xaxes(range=[-0.5, 0.5], row=1, col=4) 
fig.update_xaxes(range=[0, 100], row=2, col=2) 


fig.show()



"""### **Summery Statistics of outliers in each columns**"""

def get_limit(col):

  perct_25 = col.quantile(0.25)
  perct_75 = col.quantile(0.75)

  IQR = perct_75 - perct_25

  upper_limit = perct_75 + (1.5 * IQR)
  lower_limit = perct_25 - (1.5 * IQR)

  return upper_limit, lower_limit

cols_wth_outl = []

for col in num_colms:

  data = g_c_no_null[col]

  upper_bound, lower_bound = get_limit(data)
   
  outliers = data[(data < lower_bound) | (data > upper_bound)]

  outlier_count = len(outliers)
  total_count = len(data)
  outlier_percent = outlier_count/total_count * 100

  print(f"{col} Percentage : {outlier_percent : .2f}%")

  if outlier_percent != 0:

    cols_wth_outl.append(col)

    mean_outliers = np.mean(outliers)
    median_outliers = np.median(outliers)
    std_outliers = np.std(outliers)

    print(f"{col} Mean       : {mean_outliers : .2f}")
    print(f"{col} Median     : {median_outliers : .2f}")
    print(f"{col} Std        : {std_outliers : .2f}")

  print()

"""#### **Columns Having Outliers**"""

cols_wth_outl

g_c_no_null[cols_wth_outl].describe()

"""#### **Destribution of outlier columns:**"""

def outlier_dist_plt(df, cols_wth_outl, title, rows = 2, cols = 3, break_point = -1):

  fig = sp.make_subplots(rows=rows, cols=cols)


  idx = 0

  for r in range(1, 3):

    for c in range(1, 4):

      col = cols_wth_outl[idx]
      idx += 1

      # trace = go.Box(x  = df[col], histnorm='density')
      trace = go.Box(x = df[col], orientation='h', boxpoints='suspectedoutliers', boxmean=True)

      fig.add_trace(trace, row = r, col = c)

      fig.update_xaxes(title = col, row = r, col = c)

      if idx == break_point: break



  # fig.update_xaxes(range = [0, 100000], row = 1, col = 1)
  # fig.update_xaxes(range = [0, 1000], row = 1, col = 3)
  # fig.update_xaxes(range = [0, 500], row = 2, col = 1)

  fig.update_yaxes(showticklabels=False)
  # fig.update_yaxes(range = [0, 100], row = 2, col = 3)


  fig.update_layout(
      title={
          'text': title,
          'y': 0.989,
          'x': 0.5,
          'xanchor': 'center',
          'yanchor': 'top',
          'font' : {
              'color' : '#393646',
              'family' : 'Bold', 
              'size' : 26
          } 
          
      },

      
      showlegend = False, 
      height = 800,
      width = 1200
  )


  fig.show()

outlier_dist_plt(g_c_no_null, cols_wth_outl, 'Destributoin of numeric columns Having Outliers', break_point = 5)

num_col_dest(3, 2, g_c_no_null, cols_wth_outl, "Destribution of Numerical Columns Before handling Outliers", break_point = 5, height = 800,width= 1200)

"""#### **Using Capping Method to handle outliers**"""

def get_limit(col):

  perct_25 = col.quantile(0.25)
  perct_75 = col.quantile(0.75)

  IQR = perct_75 - perct_25

  upper_limit = perct_75 + (1.5 * IQR)
  lower_limit = perct_25 - (1.5 * IQR)

  return upper_limit, lower_limit


g_c_no_outl = g_c_encode.copy()

# Iterate through columns with outliers
for col in cols_wth_outl:

  upper_limit, lower_limit = get_limit(g_c_no_outl[col])

  # Replace values outside of the upper and lower limits with the respective limit
  g_c_no_outl[col] = np.where(
      g_c_no_outl[col] > upper_limit, 
      upper_limit,
      np.where(
          g_c_no_outl[col] < lower_limit,
          lower_limit,
          g_c_no_outl[col]
      )
  )

g_c_no_null[num_colms].describe()

g_c_no_outl[num_colms].describe()

"""#### **Plotting the destribution after handling outliers**"""

outlier_dist_plt(g_c_no_outl, cols_wth_outl, 'Box Plot Destributoin of numeric columns After Handling Outliers', break_point = 5)

num_col_dest(3, 2, g_c_no_outl, cols_wth_outl, "Destribution of Numerical Columns After handling Outliers", break_point = 5, height = 800,width= 1200)

"""### **Standardizing the features with extreme change after handling the outliers**"""

cols_to_stdz = ['mins_beerdrinking_year', 'mins_exercising_year', 'works_hours']


g_c_stdz_oult = g_c_no_outl.copy()


for col in cols_to_stdz:

  g_c_stdz_oult[col] = (g_c_stdz_oult[col] - g_c_stdz_oult[col].mean()) / g_c_stdz_oult[col].std()

g_c_no_outl[cols_to_stdz].describe()

g_c_stdz_oult[cols_to_stdz].describe()

"""### **Variance of these features Before Standardized**"""

for col in cols_to_stdz:

  var = g_c_no_outl[col].var()

  print(f"Variance of {col} : {var}")

"""**Top 10 Values counts of unique numerical value of these features before handling outliers**"""

for col in cols_to_stdz:

  print(g_c_no_null[col].value_counts().iloc[:10])
  print()

"""### **Final DataFrame After Preprocessing**"""

feature_to_select = ['user_id', 'age', 'office_jobs', 'manual_jobs', 'sales_jobs',
                     'service_jobs', 'Tech_jobs', 'workclass_private',
                     'workclass_government', 'salary', 'education_rank',
                     'marital-status_Divorced', 'marital-status_Married',
                     'marital-status_Widowed', 'race_caucasian', 'sex_Male',
                     'tea_per_year', 'coffee_per_year', 'great_customer_class']
                     
g_c_pre_process = g_c_no_outl[feature_to_select].copy()

num_colms = ['age', 'salary', 'tea_per_year', 'coffee_per_year']

catg_colms = ['office_jobs', 'manual_jobs', 'sales_jobs', 'service_jobs', 'Tech_jobs', 'workclass_private', 
              'workclass_government', 'education_rank', 'marital-status_Divorced', 'marital-status_Married', 'marital-status_Widowed', 
              'race_caucasian', 'sex_Male', 'great_customer_class']

g_c_pre_process.shape

g_c_pre_process.head()

"""## **Destribution of numerical features with respect to great_customer_class**"""

def box_plot(df, relevent_colms, rows = 2, cols = 2) -> None:
  fig = sp.make_subplots(rows=rows, cols=cols, vertical_spacing = 0.17, horizontal_spacing = 0.1)

  idx = 0

  for r in range(1, 3):
    for c in range(1, 3):

      col = relevent_colms[idx]
      idx+=1


      trace1 = go.Box(x=df[df.great_customer_class == 0][col], name='No', orientation='h', boxpoints='suspectedoutliers')
      trace2 = go.Box(x=df[df.great_customer_class == 1][col], name='Yes', orientation='h', boxpoints='suspectedoutliers')

      fig.add_trace(trace1, row=r, col=c)
      fig.add_trace(trace2, row=r, col=c)


      fig.update_yaxes(title_text='Great Customer Class', row=r, col=c)
      fig.update_xaxes(title_text=col, row=r, col=c)
  

  fig.update_xaxes(range = [0, 500], row=2, col=1)

  fig.update_layout(
      title={
          'text': 'Relationship between Numerical Features and Great Customer Class',
          'y': 0.99,
          'x': 0.5,
          'xanchor': 'center',
          'yanchor': 'top',
          'font' : {
              'color' : '#393646',
              'family' : 'Bold', 
              'size' : 26
          } 
          
      },
      showlegend = False, 
      height = 800,
      width = 1200
  )

  fig.show()

box_plot(g_c_pre_process, num_colms)

"""### **Handling the outliers of the numerical features with respect to great_customer_class**"""

categories = [0, 1]

for col in num_colms:
    for cat in categories:

        subset = g_c_pre_process[g_c_pre_process['great_customer_class'] == cat][col]
        
        q1 = subset.quantile(0.25)
        q3 = subset.quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - (1.5 * iqr)
        upper_bound = q3 + (1.5 * iqr)
        
        subset[subset < lower_bound] = lower_bound
        subset[subset > upper_bound] = upper_bound
        
        g_c_pre_process.loc[g_c_pre_process['great_customer_class'] == cat, col] = subset

catg_colms

g_c_pre_process[catg_colms].head()

"""### **Destribution of these features after handling the outliers**"""

box_plot(g_c_pre_process, num_colms)

def target_category_hist(df, title = "Relationship between Categorical Features and Great Customer Class", break_point = 13, rows = 3, cols = 5,  ):
  
  fig = make_subplots(rows=rows, cols=5, subplot_titles=catg_colms[:-1], horizontal_spacing = 0.1)

  idx = 0

  great_customer = df[df['great_customer_class'] == 1]
  not_great_customer = df[df['great_customer_class'] == 0]

  for r in range(1, rows + 1):
    for c in range(1, cols + 1):

      col = catg_colms[idx]
      idx += 1

      fig.add_trace(
          go.Histogram(x=df[col], y=great_customer['great_customer_class']),
          row=r, col=c

      )
      fig.add_trace(
          go.Histogram(x=df[col], y=not_great_customer['great_customer_class']),
          row=r, col=c

      )
      sorted_values = sorted(df[col].unique())
      fig.update_xaxes(type='category',categoryorder='array', categoryarray=sorted_values, row=r, col=c)
      fig.update_yaxes(title = "Great Customer Class", row=r, col=c)

      if idx == 13:
        break


  fig.update_layout(
      showlegend=False,
      height=800,
      width=1600,
      title_text=title,
      title_x = 0.5,
      title_y = 0.98,
      title_font_size = 22
  )


  fig.show()


target_category_hist(g_c_pre_process)

"""## **Statistical test to select feature**

#### **Doing Anova Test to numeric columns to determine the significance of predicting if a customer is great_customer or not**

#### **Creating my own Anova Function**
"""

def anova_one(*categories) -> List[float]:

  # Flatten all the categories into one 1D array
  # grand = np.array(categories)
  # grand = grand.flatten()
  grand = []

  for outer in categories:
    for elm in outer:
      grand.append(elm)

  num_of_condition = len(categories)
  total_num_of_data = len(grand)

  # for catg in categories:
  #   total_num_of_data += len(catg)

  df_bw = num_of_condition - 1
  df_wtn = total_num_of_data - num_of_condition

  # Critical Value
  f_crit = f.ppf(q=1-.05, dfn=df_bw, dfd=df_wtn)
  
  grand_mean = np.mean(grand)
  mean_arr = [np.mean(x) for x in categories] 

  SST_numerator = grand

  # Sum of Squares Total Σ(x - x̄)^2
  SST = np.sum(np.apply_along_axis(lambda x: np.power(x - grand_mean, 2), 0, SST_numerator))

  # Sum of Squares Withing Σ(x1 - x̄1)^2 + Σ(x2 - x̄2)^2 + ... ... ... + Σ(xn - x̄n)^2
  SSW = 0
  temp = categories

  for catg in temp:

    catg_mean = np.mean(catg)

    SSW += np.sum(np.apply_along_axis(lambda x: np.power(x - catg_mean, 2), 0, catg))

  SSW = SSW

  # Sum of Squares Between
  SSBW = SST - SSW

  # Mean Squares Between
  MSBW =SSBW / df_bw

  # Mean Squares Within
  MSW = SSW / df_wtn

  f_stat = MSBW / MSW


  p_value = f.sf(f_stat, df_bw, df_wtn)

  return f_crit, f_stat, p_value

"""#### **Implementing Anova test with both our and built in functions and comparing it**"""

# Extracting the numerical columns

HC_anova = []
anova = []

for col in num_colms[ :- 1]:

  col_0 = g_c_pre_process[g_c_pre_process["great_customer_class"] == 0][col]
  col_1 = g_c_pre_process[g_c_pre_process["great_customer_class"] == 1][col]


  f_stat, p_value = f_oneway(col_0, col_1)

  anova.append([col, f_stat, p_value])

  _, f_stat, p_value = anova_one(col_0, col_1)

  HC_anova.append([col, f_stat, p_value])

# col = 'mins_beerdrinking_year'

# col_0 = g_c_no_outl[g_c_no_outl["great_customer_class"] == 0]
# col_1 = g_c_no_outl[g_c_no_outl["great_customer_class"] == 1][col]

# g_c_no_outl.head()

"""#### **Comparing my anova function to original anova**



"""

for i in range(len(anova)):
  print(anova[i][0])
  print(f"stat = {anova[i][1]}, p_value = {anova[i][2]}")
  print(f"my_stat = {HC_anova[i][1]}, my_p_value = {HC_anova[i][2]}")
  print()

"""### **Doing Chi Square Test to categorical columns to determine the significance of predicting price_range**

#### **Creating our Own Chi Square Function**
"""

def cross_tab(ind, dep):
  
  # Fetching the rows and columns for the table

  new_df = pd.DataFrame({
      'ind' : ind,
      'dep' : dep
  })


  rows = ind.unique()
  cols = dep.unique()
  cols.sort()
  cols

  table = []
  
  for row in rows:
    c = []
    for col in cols:
      slices = len(new_df[(new_df['ind'] == row) & (new_df['dep'] == col)])
      c.append(slices)

    table.append(c.copy())
    c = c.clear()

  table = pd.DataFrame(table, columns = cols, index = rows)

  return table
  


def to_expected(observe):

  cols = observe.columns
  rows = observe.index

  grand_ttl = np.sum(np.sum(observe))

  expected = observe.copy()

  for row in rows:
    for col in cols:
      row_ttl = np.sum(observe.loc[row])
      col_ttl = np.sum(observe[col])
      
      expected.loc[row, col] = (row_ttl * col_ttl) / grand_ttl
  
  return expected

# print(to_expected(observe))
      

def chi_2_t(observed, expected):

  chi_square = np.sum(np.sum(np.power(observe - expected, 2) / expected))



  row_count = len(observed.index) - 1
  col_count = len(observed.columns) - 1
  
  df = row_count * col_count

  p_value = 1 - chi2.cdf(chi_square, df)

  return chi_square, p_value

"""### **Comparing my Chi2 function with built in functions**



"""

observe = cross_tab(g_c_pre_process['office_jobs'], g_c_pre_process['great_customer_class'])
expected = to_expected(observe) 

chi_square,p_value,_,_ = chi2_contingency(observe, correction=False)
HC_chi_square, HC_p_values = chi_2_t(observe, expected)

print(chi_square,p_value)
print(HC_chi_square, HC_p_values)

# Fetching the categorical columns
n = len(catg_colms)

HC_chi_2 = []
chi_2 = []

for col in catg_colms[:-1]:

  observe = cross_tab(g_c_pre_process[col], g_c_pre_process['great_customer_class'])
  expected = to_expected(observe) 

  chi_square,p_value,_,_ = chi2_contingency(observe, correction=False)
  HC_chi_square, HC_p_values = chi_2_t(observe, expected)

  chi_2.append([col, chi_square, p_value])
  HC_chi_2.append([col, HC_chi_square, HC_p_values])

for i in range(len(chi_2)):
  print(HC_chi_2[i][0]) 
  print(f"stat    = {chi_2[i][1]},  p_value    = {chi_2[i][2]}")
  print(f"my_stat = {HC_chi_2[i][1]}, my_p_value = {HC_chi_2[i][2]}")
  print()

p_values_num = {anova[i][0] : anova[i][2] for i in range(len(anova))}
p_values_catg = {HC_chi_2[i][0] : HC_chi_2[i][2] for i in range(len(HC_chi_2))}

p_values  = dict(p_values_num)
p_values.update(p_values_catg)

sort_p_values = sorted(p_values.items(), key = lambda x: x[1])
sort_p_values

# footballers_goals = {'Eusebio': 120, 'Cruyff': 104, 'Pele': 150, 'Ronaldo': 132, 'Messi': 125}

# sorted_footballers_by_goals = sorted(footballers_goals.items(), key=lambda x:x[1])
# print(sorted_footballers_by_goals)

"""Based on the 0.5 significance level:

The analysis revealed that several features are highly significant predictors of the 'great_customer_class', including 'office_jobs', 'Tech_jobs', 'workclass_private', 'workclass_government', 'education_rank', 'marital-status_Divorced', and 'marital-status_Married'. 

Additionally, features such as 'salary', 'tea_per_year', 'service_jobs', 'sex_Male', 'sales_jobs', and 'age' showed moderate significance.

#**Relevant Exploratory Data Analysis**
"""

df = g_c_no_outl.copy()

df.isnull().mean() * 100

df.head()

df[num_colms].describe()

df.info()

"""#### **1. Distribution of age among the great customers**"""

great_customer = df[df['great_customer_class'] == 1]

fig = px.histogram(great_customer, x='age', nbins=20, color = "sex_Male")
fig.update_layout(
    xaxis_title='Age', yaxis_title='Count',
    title = {
        'text' : 'Distribution of Age Among Great Customers',
        'x' : 0.5, 
        'y' : 0.98,
        'font_family' : 'bold',
        'font_size' : 22
    },
    height = 600,
    width = 1200
)
fig.show()

"""#### **2. Is there a correlation between an individual's age and salary?**"""

fig, axes = plt.subplots(1, 1, figsize=(10, 5))

corr_matrix = df[['age', 'salary']].corr()

sns.heatmap(corr_matrix, ax= axes, annot=True, cmap=plt.cm.CMRmap_r)

plt.show()

fig = px.scatter(df, x = 'age', y = 'salary', color = 'education_rank')

corr_coef = df['age'].corr(df['education_rank'])

fig.update_layout(
    title={
        'text' : f'Relation Between Individuals Age and Salary {corr_coef:.2f}',
        'x'    : 0.5,
        'y'    : 0.93,
        'font_family' : 'bold',
        'font_size' : 22

    },
    height = 500,
    width = 1000
)

fig.show()

"""#### From the above correlation matrix and the plot we can conclude that there is very week positive correlation between age and salary. As a result we can say both features are not similar and each provide unique stats

#### **3. Is there a significant difference in the average salary between males and females**
"""

male_salary = df[df['sex_Male'] == 1]['salary']
female_salary = df[df['sex_Male'] == 0]['salary']

t_stat, p_value = ttest_ind(female_salary, male_salary, equal_var=False)

t_stat, p_value

"""#### The t-test performed indicates a statistically significant difference between the average salaries of males and females. The obtained t-statistic of 23.66 and p-value of 7.33e-117 suggest that the difference in salary between males and females is not likely due to chance. Therefore, we reject the null hypothesis and conclude that there is a significant difference in the average salary between males and females.

#### **4. What is the most common occupation among individuals who are self-employed**
"""

self_employed = df[(df['workclass_private'] == 0) & (df['workclass_government'] == 0)]


office_jobs = self_employed[self_employed.office_jobs == 1]
sales_jobs = self_employed[self_employed.sales_jobs == 1]
manual_jobs = self_employed[self_employed.manual_jobs == 1]
service_jobs = self_employed[self_employed.service_jobs == 1]
Tech_jobs = self_employed[self_employed.Tech_jobs == 1]


data = pd.DataFrame({
    'Occupation' : [len(office_jobs),len(sales_jobs),len(manual_jobs),len(service_jobs), len(Tech_jobs)]

}, index = ['office_jobs', 'manual_jobs', 'sales_jobs', 'service_jobs', 'Tech_jobs']).sort_values(by = 'Occupation', ascending = False)

trace = go.Bar(x=data.index, y=data['Occupation'], texttemplate='%{y:.2s}', textposition='auto')



fig = go.Figure(data=[trace])

fig.update_xaxes()

fig.update_layout(
    title = {
        'text' : 'Occupations of Self-Employed Individuals',
        'x' :  0.5,
        'y' : 0.95,
        'font_family' : 'bold',
        'font_size' : 22
    }
)

fig.show()

"""#### Based on the bar plot, it appears that Tech_jobs is the most common occupation among self-employed individuals. This may suggest that there is a growing trend towards self-employment in the technology industry.

#### **5. Are individuals who work in government jobs more likely to consume coffee or tea, and how does this vary by race?**
"""

government_df = df[df['workclass_government'] == 1]
race_coffee_tea = government_df.groupby('race_caucasian')[['coffee_per_year', 'tea_per_year']].mean()


trace1 = go.Bar(x=race_coffee_tea.index, y=race_coffee_tea['coffee_per_year'], name='Coffee')
trace2 = go.Bar(x=race_coffee_tea.index, y=race_coffee_tea['tea_per_year'], name='Tea')

data = [trace1, trace2]

fig = go.Figure(data=data)

fig.update_layout(
    title={
        'text' : 'Average Coffee and Tea Consumption of Government Workers by Race',
        'x'    : 0.5,
        'y'    : 0.93,
        'font_family' : 'bold',
        'font_size' : 22

    },
    xaxis={
        'title':'Race', 
        'tickvals':[0, 1], 
        'ticktext':['Non-Caucasian', 'Caucasian']
    },
    yaxis={
        'title':'Average Consumption'
    }
       
)
fig.show()

"""From above plot we can see that, it appears that both non-Caucasian and Caucasian individuals working in government jobs consume coffee more frequently than tea. However, the average coffee consumption among Caucasians appears to be slightly higher than non-Caucasians, with a mean value of 283.9708 compared to 260.54. On the other hand, non-Caucasians appear to consume slightly more tea on average, with a mean value of 204.3896 compared to 208.8661 for Caucasians. However, the difference in tea consumption between the two racial groups is much smaller than the difference in coffee consumption. Overall, it seems that both coffee and tea are popular beverages among government workers, regardless of their race.

#### **6. Are individuals who are married more likely to have a higher education rank than those who are unmarried, and does this differ by race?**
"""

married = df[df['marital-status_Married'] == 1]
unmarried = df[(df['marital-status_Married'] == 0) & (df['marital-status_Divorced'] == 0) & (df['marital-status_Widowed'] == 0)]

married_edu_rank = married.groupby('race_caucasian')['education_rank'].apply(lambda x: x.mode()[0]).reset_index()
unmarried_edu_rank = unmarried.groupby('race_caucasian')['education_rank'].apply(lambda x: x.mode()[0]).reset_index()

trace1 = go.Bar(x=married_edu_rank['race_caucasian'], y=married_edu_rank['education_rank'], name='Married')
trace2 = go.Bar(x=unmarried_edu_rank['race_caucasian'], y=unmarried_edu_rank['education_rank'], name='Unmarried')

data = [trace1, trace2]

fig = go.Figure(data=data)
fig.update_layout(
    
    title={
        'text' : ' Most Common Education Rank of Married and Unmarried Individuals by Race',
        'x'    : 0.5,
        'y'    : 0.93,
        'font_family' : 'bold',
        'font_size' : 22

    },
    xaxis={
        'title'   :'Race',
        'tickvals':[0, 1], 
        'ticktext':['Non-Caucasian', 'Caucasian']
    },
    yaxis={
        'title':'Mode Education Rank'
    }
)

fig.show()

"""From the plot, it is difficult to draw any conclusions about the relationship between marital status and education rank, as all four data points are exactly the same. All four data points have a race value of either Caucasian or non-Caucasian, a marital status value of either married or unmarried, and an education rank value of 9.

Therefore, we can conclude that there is no observed difference in education rank based on marital status within each racial group in the data being presented.

#### **7. Average salary across different job types (office, manual, sales, service, tech)**
"""

jobs = ['office_jobs', 'manual_jobs', 'sales_jobs', 'service_jobs', 'Tech_jobs']

avg_salary = []

for job in jobs:

  avg_sal = np.mean(df[df[job] == 1]['salary'])

  avg_salary.append(float(f"{avg_sal:.2f}"))

salary_accross_jobs = pd.DataFrame({
    'AVG_salary' : avg_salary
}, index = jobs)

salary_accross_jobs

colors = ['#088395', '#FF6969', '#0A4D68', '#41644A', '#FF8400']

fig = px.bar(salary_accross_jobs, x='AVG_salary', text_auto = True, color = colors)


fig.update_layout(
    
    title={
        'text' : 'Average Salary by Job Type',
        'x'    : 0.5,
        'y'    : 0.97,
        'font_family' : 'bold',
        'font_size' : 22

    },
    showlegend = False,
    yaxis={
        'title':'Jobs'
    },
    height = 500,
    width = 1200,
    xaxis_tickformat = '.2f'
)

fig.show()

"""
From above plot, we can see that the average salary varies slightly across different job types. Among the job types listed, the highest average salary is for manual jobs at 45118.59, followed closely by sales jobs at 45296.86 and office jobs at 44820.61. Service jobs have a slightly lower average salary at 43531.55.

The average salary for tech jobs is much lower than the other job types listed, at 24007.83. This could indicate that the tech jobs in this dataset are lower-skilled or entry-level positions with lower salaries, or that the dataset only includes a small number of tech jobs.
"""

# Numerical COlumns: 'age', 'salary', 'tea_per_year', 'coffee_per_year'

# Categorical Columns: 'user_id', 'office_jobs', 'manual_jobs', 'sales_jobs', 'service_jobs', 'Tech_jobs', 'workclass_private', 'workclass_government', 'education_rank', 'marital-status_Divorced', 'marital-status_Married', 'marital-status_Widowed', 'race_caucasian', 'sex_Male', 'great_customer_class']

"""#### **8. How does the great_customer_class vary across different categories such as race and sex?**"""

great_customer = df[df['great_customer_class'] == 1]

group_df = great_customer.groupby(['race_caucasian', 'sex_Male'])['great_customer_class'].count().reset_index()


fig = px.histogram(great_customer, x="race_caucasian", y="great_customer_class",
             color='sex_Male',
             barmode='group',
             histfunc='sum',
             height=400,
             text_auto=True)


fig.update_layout(
    
     title={
        'text' : 'Great Customer Class Count by Race and Gender',
        'x'    : 0.5,
        'y'    : 0.97,
        'font_family' : 'bold',
        'font_size' : 22

    },
    xaxis={
        'title':'Race', 
        'tickvals':[0, 1], 
        'ticktext':['Non-Caucasian', 'Caucasian']
    },
    yaxis = {
        'title' : 'Count of Great Customer Class'
    },
    height = 500, 
    width = 1200
)


fig.show()

"""From the plot, we can see that the great_customer_class appears to vary across different categories of sex and race. Among non-Caucasian customers, the sum of the great_customer_class is 24 for females (sex_Male=0) and 16 for males (sex_Male=1). This suggests that female non-Caucasian customers may be more likely to have a higher great_customer_class than male non-Caucasian customers. However, it's important to note that the difference in great_customer_class between the two groups is relatively small.

Among Caucasian customers, the sum of the great_customer_class is much higher overall, with a sum of 930 for males (sex_Male=1) and 138 for females (sex_Male=0). This suggests that Caucasian male customers may be more likely to have a higher great_customer_class than female Caucasian customers.

#### **9. Distribution of office_jobs, manual_jobs, sales_jobs, service_jobs, and Tech_jobs across different categories such as race and sex?**
"""

job_categories = ["office_jobs", "manual_jobs", "sales_jobs", "service_jobs", "Tech_jobs"]

office = great_customer[["race_caucasian", "sex_Male", "office_jobs"]]

fig = px.histogram(office, x=["race_caucasian", "sex_Male"], y="office_jobs",
             color='sex_Male',
             barmode='group',
             histfunc='sum',
             height=400,
             text_auto=True)

fig.update_xaxes(title = "office_jobs")
fig.update_xaxes(title = "Race")

fig.update_layout(
    title = {
        'text' : 'Destributoin of difference jobs across race and gender',
        'font_family' : 'bold',
        'font_size' : 22,
        'x' : 0.5,
        'y' : 0.98
    },
    height = 300
)

fig.show()



manual = great_customer[["race_caucasian", "sex_Male", "manual_jobs"]]
fig = px.histogram(manual, x=["race_caucasian", "sex_Male"], y="manual_jobs",
             color='sex_Male',
             barmode='group',
             histfunc='sum',
             height=400,
             text_auto=True)

fig.update_xaxes(title = "Race")
fig.update_xaxes(title = "manual_jobs")


fig.update_layout(height = 300)

fig.show()

sales = great_customer[["race_caucasian", "sex_Male", "sales_jobs"]]
fig = px.histogram(sales, x=["race_caucasian", "sex_Male"], y="sales_jobs",
             color='sex_Male',
             barmode='group',
             histfunc='sum',
             height=400,
             text_auto=True)

fig.update_xaxes(title = "Race")
fig.update_xaxes(title = "sales_jobs")
fig.update_layout(height = 300)


fig.show()

service = great_customer[["race_caucasian", "sex_Male", "service_jobs"]]
fig = px.histogram(service, x=["race_caucasian", "sex_Male"], y="service_jobs",
             color='sex_Male',
             barmode='group',
             histfunc='sum',
             height=400,
             text_auto=True)

fig.update_xaxes(title = "Race")
fig.update_xaxes(title = "service_jobs")

fig.update_layout(height = 300)

fig.show()

Tech = great_customer[["race_caucasian", "sex_Male", "Tech_jobs"]]
fig = px.histogram(Tech, x=["race_caucasian", "sex_Male"], y="Tech_jobs",
             color='sex_Male',
             barmode='group',
             histfunc='sum',
             height=400,
             text_auto=True)

fig.update_xaxes(title = "Race")
fig.update_xaxes(title = "Tech_jobs")

fig.update_layout(height = 300)

fig.show()

"""From the above plot, we can see the distribution of office_jobs, manual_jobs, sales_jobs, service_jobs, and Tech_jobs across different categories such as race and sex.

For office_jobs, we can see that the sum of office_jobs is higher for males (sex_Male=1) than females (sex_Male=0) for both non-Caucasian and Caucasian groups, with a particularly large difference for the non-Caucasian group.

For manual_jobs, we can see that the sum of manual_jobs is also higher for males (sex_Male=1) than females (sex_Male=0) for both non-Caucasian and Caucasian groups, although the difference is not as large as for office_jobs.

For sales_jobs, we can see that the sum of sales_jobs is higher for males (sex_Male=1) than females (sex_Male=0) for both non-Caucasian and Caucasian groups, although again the difference is not as large as for office_jobs.

For service_jobs, we do not have any information in the provided plot.

For Tech_jobs, we can see that the sum of Tech_jobs is higher for males (sex_Male=1) than females (sex_Male=0) for both non-Caucasian and Caucasian groups, although again the difference is not as large as for office_jobs.

#### **10. The proportion of great customers in each marital status category**
"""

def marital_status_proportion(df):
  unmarried = df[(df['marital-status_Married'] == 0) & (df['marital-status_Divorced'] == 0) & (df['marital-status_Widowed'] == 0)]
  married = df[df['marital-status_Married'] == 1]
  widowed = df[df['marital-status_Widowed'] == 1]
  divorce = df[df['marital-status_Divorced'] == 1]


  total_len = df.shape[0]

  unmarried_proportion = float(f"{(len(unmarried) / total_len) * 100 : .2f}")
  married_proportion   = float(f"{(len(married) / total_len)   * 100: .2f}")
  widowed_proportion   = float(f"{(len(widowed) / total_len)   * 100: .2f}")
  divorce_proportion   = float(f"{(len(divorce) / total_len)   * 100: .2f}")

  labels = ['Unmarried', 'Married', 'Widowed', 'Divorced']
  proportions = [unmarried_proportion, married_proportion, widowed_proportion, divorce_proportion]
  colors = ['#F15A59', '#393646', '#4287f5', '#FFA500'] 

  fig = px.bar(
      x=labels,
      y=proportions,
      labels={'x': 'Marital Status', 'y': 'Poportions (%)'},
      color = colors,
      text_auto = True
      
  )

  fig.update_layout(
      title={
          'text' : 'Proportion of Great Customers by Marital Status',
          'x'    : 0.5,
          'y'    : 0.97,
          'font_family' : 'bold',
          'font_size' : 22

      },
      yaxis_tickformat='.2f',
      showlegend = False,
      height = 500, 
      width = 1200
  )

  fig.show()

marital_status_proportion(great_customer)

"""The plot shows the proportion of great customers in each marital status category. Among the four categories, married customers have the highest proportion of great customers with 86.13%, followed by divorced customers with 7.19%. Unmarried customers have a proportion of 5.82%, while windowed customers have the lowest proportion with 0.86%. This information could be useful for businesses that target customers based on their marital status to adjust their marketing strategies to attract more customers who are more likely to be great customers. However, it's important to keep in mind that this data is based on a specific sample and may not be representative of the population as a whole.

#### **11. The proportion of not great customers in each marital status category**
"""

not_great_customer = df[df['great_customer_class'] == 0]
marital_status_proportion(not_great_customer)

"""The plot shows the proportion of not great customers in each marital status category. Among the four categories, divorced customers have the highest proportion of not great customers with 52.62%, followed by unmarried customers with 24.72%. Married customers have a proportion of 20.54% of not great customers, while widow customers have the lowest proportion with 2.12%. This information could be useful for businesses that target customers based on their marital status to adjust their marketing strategies to avoid attracting customers who are less likely to be great customers. However, it's important to keep in mind that this data is based on a specific sample and may not be representative of the population as a whole."""

catg_colms

num_colms

